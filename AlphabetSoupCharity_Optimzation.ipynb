{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlphabetSoupCharity_Optimzation.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udhPPtU4SAr6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import our dependencies\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "#  Import and read the charity_data.csv.\n",
        "import pandas as pd \n",
        "application_df = pd.read_csv('gdrive/My Drive/Colab Notebooks/Resources/charity_data.csv')\n",
        "application_df.head()\n",
        " \n",
        " "
      ],
      "metadata": {
        "id": "XkR4xkyvSQj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "HIl1sAbwS6ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.\n",
        "application_df.drop(['EIN', 'NAME'], axis=1, inplace=True)\n",
        "application_df.head()\n"
      ],
      "metadata": {
        "id": "kfDUBscgS-51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the number of unique values in each column.\n",
        "application_df.nunique()\n"
      ],
      "metadata": {
        "id": "XweRt6lyTAKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at APPLICATION_TYPE value counts for binning\n",
        "app_type_value_count = application_df[\"APPLICATION_TYPE\"].value_counts()\n",
        "app_type_value_count\n"
      ],
      "metadata": {
        "id": "_GC3OACTTGuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the value counts of APPLICATION_TYPE\n",
        "app_type_value_count.plot.density()\n"
      ],
      "metadata": {
        "id": "aXK0uDC7TJms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine which values to replace if counts are less than ...?\n",
        "replace_application = list(app_type_value_count[app_type_value_count < 200].index)\n",
        "\n",
        "# Replace in dataframe\n",
        "for app in replace_application:\n",
        "    application_df.APPLICATION_TYPE = application_df.APPLICATION_TYPE.replace(app,\"Other\")\n",
        "    \n",
        "# Check to make sure binning was successful\n",
        "application_df.APPLICATION_TYPE.value_counts()\n"
      ],
      "metadata": {
        "id": "AxLYA87PTQ7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at CLASSIFICATION value counts for binning\n",
        "classification_value_count = application_df[\"CLASSIFICATION\"].value_counts()\n",
        "classification_value_count\n"
      ],
      "metadata": {
        "id": "b6z1QEpcTkvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the value counts of CLASSIFICATION\n",
        "classification_value_count.plot.density()\n"
      ],
      "metadata": {
        "id": "DZm8Bx8UTmqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine which values to replace if counts are less than ..?\n",
        "replace_class = list(classification_value_count[classification_value_count < 200].index)\n",
        "\n",
        "# Replace in dataframe\n",
        "for cls in replace_class:\n",
        "    application_df.CLASSIFICATION = application_df.CLASSIFICATION.replace(cls,\"Other\")\n",
        "    \n",
        "# Check to make sure binning was successful\n",
        "application_df.CLASSIFICATION.value_counts()\n"
      ],
      "metadata": {
        "id": "tQlUhjpuToal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate our categorical variable lists\n",
        "application_cat = application_df.dtypes[application_df.dtypes == \"object\"].index.tolist()\n",
        "\n",
        "\n",
        "# Check the number of unique values in each column\n",
        "application_df[application_cat].nunique()\n"
      ],
      "metadata": {
        "id": "V2rX_KtJTp9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a OneHotEncoder instance\n",
        "enc = OneHotEncoder(sparse=False)\n",
        "\n",
        "# Fit and transform the OneHotEncoder using the categorical variable list\n",
        "encode_df = pd.DataFrame(enc.fit_transform(application_df[application_cat]))\n",
        "\n",
        "# Add the encoded variable names to the dataframe\n",
        "encode_df.columns = enc.get_feature_names(application_cat)\n",
        "encode_df.head()\n"
      ],
      "metadata": {
        "id": "ibaL8jnETsL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge one-hot encoded features and drop the originals\n",
        "encoded_application_df = application_df.merge(encode_df,left_index=True, right_index=True).drop(columns = application_cat)\n",
        "encoded_application_df.head()\n",
        " "
      ],
      "metadata": {
        "id": "B_mOxF9hTuIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our preprocessed data into our features and target arrays\n",
        "y = encoded_application_df[\"IS_SUCCESSFUL\"].values\n",
        "X = encoded_application_df.drop(columns=[\"IS_SUCCESSFUL\"], axis=1).values\n",
        "\n",
        "# Split the preprocessed data into a training and testing dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n"
      ],
      "metadata": {
        "id": "fscLHA1WTwzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a StandardScaler instances\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the StandardScaler\n",
        "X_scaler = scaler.fit(X_train)\n",
        "\n",
        "# Scale the data\n",
        "X_train_scaled = X_scaler.transform(X_train)\n",
        "X_test_scaled = X_scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "KSBMWWzPTyzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
        "number_input_features = len(X_train_scaled[0])\n",
        "hidden_nodes_layer1 = 80\n",
        "hidden_nodes_layer2 = 30\n",
        "\n",
        "nn = tf.keras.models.Sequential()\n",
        "\n",
        "# First hidden layer\n",
        "nn.add(\n",
        "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
        ")\n",
        "\n",
        "# Second hidden layer\n",
        "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
        "\n",
        "# Output layer\n",
        "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "# Check the structure of the model\n",
        "nn.summary()\n"
      ],
      "metadata": {
        "id": "AVW03rClT0v1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import checkpoint dependencies\n",
        "import os\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Define the checkpoint path and filenames\n",
        "os.makedirs(\"checkpoints/\",exist_ok=True)\n",
        "checkpoint_path = \"checkpoints/weights.{epoch:02d}.hdf5\"\n"
      ],
      "metadata": {
        "id": "fN8dS57lVMWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Create a callback that saves the model's weights every epoch\n",
        "cp_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    verbose=1,\n",
        "    save_weights_only=True,\n",
        "    save_freq='epoch')\n",
        "\n",
        "# Train the model\n",
        "fit_model = nn.fit(X_train_scaled,y_train,epochs=30,callbacks=[cp_callback])\n"
      ],
      "metadata": {
        "id": "NKKXfsbzVv4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model using the test data\n",
        "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n"
      ],
      "metadata": {
        "id": "vFRzc0geSfO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export model to HDF5 file\n",
        "nn.save(\"gdrive/My Drive/Colab Notebooks/Resources/Attempt1.h5\")\n"
      ],
      "metadata": {
        "id": "XesL3FT-WRzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attempt 2"
      ],
      "metadata": {
        "id": "XXLz_uXhYi8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
        "number_input_features = len(X_train_scaled[0])\n",
        "hidden_nodes_layer1 = 80\n",
        "hidden_nodes_layer2 = 30\n",
        "hidden_nodes_layer3 = 10\n",
        "\n",
        "\n",
        "nn = tf.keras.models.Sequential()\n",
        "\n",
        "# First hidden layer\n",
        "nn.add(\n",
        "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
        ")\n",
        "\n",
        "# Second hidden layer\n",
        "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
        "\n",
        "# Third hidden layer\n",
        "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n",
        "\n",
        "# Output layer\n",
        "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "# Check the structure of the model\n",
        "nn.summary()\n"
      ],
      "metadata": {
        "id": "1qk7rnq0Yl4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import checkpoint dependencies\n",
        "import os\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Define the checkpoint path and filenames\n",
        "os.makedirs(\"checkpoints/\",exist_ok=True)\n",
        "checkpoint_path = \"checkpoints/weights.{epoch:02d}.hdf5\"\n"
      ],
      "metadata": {
        "id": "nUFTVMS0ZJLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Create a callback that saves the model's weights every epoch\n",
        "cp_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    verbose=1,\n",
        "    save_weights_only=True,\n",
        "    save_freq='epoch')\n",
        "\n",
        "# Train the model\n",
        "fit_model = nn.fit(X_train_scaled,y_train,epochs=30,callbacks=[cp_callback])\n"
      ],
      "metadata": {
        "id": "faR_7j4TZNpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model using the test data\n",
        "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n"
      ],
      "metadata": {
        "id": "xS0M2NAOZS2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export model to HDF5 file\n",
        "nn.save(\"gdrive/My Drive/Colab Notebooks/Resources/Attempt2.h5\")\n"
      ],
      "metadata": {
        "id": "jp_ypPr9ZW3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attempt 3 - tanh"
      ],
      "metadata": {
        "id": "rUtrTdjUZ3s7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
        "number_input_features = len(X_train_scaled[0])\n",
        "hidden_nodes_layer1 = 80\n",
        "hidden_nodes_layer2 = 50\n",
        "hidden_nodes_layer3 = 30\n",
        "hidden_nodes_layer4 = 10\n",
        "\n",
        "nn = tf.keras.models.Sequential()\n",
        "\n",
        "# First hidden layer\n",
        "nn.add(\n",
        "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
        ")\n",
        "\n",
        "# Second hidden layer\n",
        "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
        "\n",
        "# Third hidden layer\n",
        "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n",
        "\n",
        "# Fourth hidden layer\n",
        "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer4, activation=\"relu\"))\n",
        "\n",
        "# Output layer\n",
        "nn.add(tf.keras.layers.Dense(units=1, activation=\"tanh\"))\n",
        "\n",
        "# Check the structure of the model\n",
        "nn.summary()\n"
      ],
      "metadata": {
        "id": "mMGJSMb_Z7ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import checkpoint dependencies\n",
        "import os\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Define the checkpoint path and filenames\n",
        "os.makedirs(\"checkpoints/\",exist_ok=True)\n",
        "checkpoint_path = \"checkpoints/weights.{epoch:02d}.hdf5\"\n"
      ],
      "metadata": {
        "id": "CzXUh3dBaYQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Create a callback that saves the model's weights every epoch\n",
        "cp_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    verbose=1,\n",
        "    save_weights_only=True,\n",
        "    save_freq='epoch')\n",
        "\n",
        "# Train the model\n",
        "fit_model = nn.fit(X_train_scaled,y_train,epochs=30,callbacks=[cp_callback])\n"
      ],
      "metadata": {
        "id": "Ke-dzUVsabEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model using the test data\n",
        "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n"
      ],
      "metadata": {
        "id": "j4b-pkkMaeyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export model to HDF5 file\n",
        "nn.save(\"gdrive/My Drive/Colab Notebooks/Resources/AlphabetSoupCharity_Optimization.h5\")\n"
      ],
      "metadata": {
        "id": "iN2-8dgbalEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MECunH40a39P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2lOH7N3oZoYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Kffy4EgQYini"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}